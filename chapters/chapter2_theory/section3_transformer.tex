\section{Μετασχηματιστές}
\label{section:transformers}

Έχοντας πλέον ορίσει την έννοια της προσοχής ως θεμελιώδη μηχανισμό στην επεξεργασία φυσικής γλώσσας, καθίσταται αναγκαία η ανάλυση της αρχιτεκτονικής του μετασχηματιστή (Transformer), η οποία αποτέλεσε καθοριστική καινοτομία στην υπολογιστική γλωσσολογία. Η συγκεκριμένη αρχιτεκτονική \cite{vaswani2017attention}, επέφερε σημαντική μεταστροφή στον τρόπο προσέγγισης των γλωσσικών μοντέλων, καταργώντας την εξάρτηση από τα επαναληπτικά νευρωνικά δίκτυα και βασιζόμενη αποκλειστικά σε μηχανισμούς προσοχής.

Οι μετασχηματιστές κατάφεραν να υπερβούν τους εγγενείς υπολογιστικούς περιορισμούς των προηγούμενων αρχιτεκτονικών \cite{cho2014learning}, ιδίως τη σειριακή φύση των LSTM και GRU δικτύων που εμπόδιζε την αποτελεσματική παραλληλοποίηση των υπολογισμών, καθώς και το πρόβλημα της υποβάθμισης των μακρινών εξαρτήσεων σε εκτεταμένες ακολουθίες \cite{bengio1994learning}. Η θεωρητική αναλυτική ικανότητα των μετασχηματιστών έχει τεκμηριωθεί από τους Chulhee Yun, et al.\cite{yun2020transformers}, οι οποίοι απέδειξαν ότι, υπό συγκεκριμένες συνθήκες, οι μετασχηματιστές μπορούν να λειτουργήσουν ως καθολικοί προσεγγιστές (universal approximators) για συναρτήσεις ακολουθιών. Δηλαδή, διαθέτουν επαρκή εκφραστική δύναμη ώστε να προσεγγίσουν οποιαδήποτε απεικόνιση μεταξύ ακολουθιών εισόδων και εξόδων, εφόσον διαθέτουν επαρκές μέγεθος και κατάλληλη παραμετροποίηση. Η αρχιτεκτονική αυτή αποτελεί τη θεωρητική και τεχνολογική βάση των σύγχρονων μεγάλων γλωσσικών μοντέλων, καθιστώντας την κατανόησή της θεμελιώδες στοιχείο για την κατανόηση της σύγχρονης τεχνητής νοημοσύνης.

\subsection{Πολυκεφαλική Προσοχή}

Η πολυκεφαλική προσοχή (multihead attention) αποτελεί εξελιγμένη μορφή του μηχανισμού αυτοπροσοχής και συνιστά θεμελειώδες στοιχείο της αρχιτεκτονικής των μετασχηματιστών \cite{amidi2024transformers}. Η βαθιά κατανόηση του συγκεκριμένου μηχανισμού κρίνεται απαραίτητη για την αντίληψη της λειτουργίας των σύγχρονων γλωσσικών μοντέλων.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images//chapter2/multihead_attention.png}
    \caption{Σχηματικό διάγραμμα πολυκεφαλικής προσοχής (προσαρμογή από \cite{amidi2024transformersimage})}
    \label{fig:multihead-attention}
\end{figure}

Σε κάθε κεφαλή προσοχής $i$, οι είσοδοι ερώτηση, κλειδί και τιμή πολλαπλασιάζονται με διαφορετικούς πίνακες βαρών $W_i^Q, W_i^K, W_i^V$ αντίστοιχα. Μέσω αυτής της διαδικασίας προκύπτουν οι πίνακες:
\[
Q_i = X W_i^Q, \quad K_i = X W_i^K, \quad V_i = X W_i^V,
\]
όπου $X$ αναπαριστά τον πίνακα εισόδων, συνήθως τα διανύσματα ενσωμάτωσης των tokens.

Στη συνέχεια εφαρμόζεται ο μηχανισμός προσοχής σύμφωνα με τη σχέση:
\begin{equation}
\text{Attention}(Q_i, K_i, V_i) = \text{softmax}\!\left(\frac{Q_i K_i^\top}{\sqrt{d_k}}\right)V_i,
\end{equation}
όπου $d_k$ αναφέρεται στη διάσταση των διανυσμάτων $K_i$. Το παραγόμενο αποτέλεσμα συνιστά έναν πίνακα $Z_i$ που εκφράζει τον τρόπο με τον οποίο κάθε token αλληλεπιδρά με τα υπόλοιπα tokens της ακολουθίας.

Η καινοτομία της πολυκεφαλικής προσοχής έγκειται στο γεγονός ότι η προαναφερθείσα διαδικασία πραγματοποιείται παράλληλα από $h$ διακριτές κεφαλές προσοχής, παράγοντας τα αποτελέσματα $Z_1, Z_2, \dots, Z_h$. Η εμπειρική έρευνα των Kevin Clark et al.\cite{clark2019does} και των Jesse Vig et al. \cite{vig2019analyzing} έχει αποκαλύψει ότι κάθε κεφαλή αναπτύσσει εξειδίκευση σε διαφορετικές γλωσσικές πτυχές. Ενδεικτικά, συγκεκριμένες κεφαλές εμφανίζουν ευαισθησία σε συντακτικές σχέσεις όπως η συμφωνία ρήματος-υποκειμένου, ενώ άλλες επικεντρώνονται σε σημασιολογικές συσχετίσεις μεταξύ εννοιολογικά συναφών όρων.

Στο τέλος της διαδικασίας, τα αποτελέσματα όλων των κεφαλών συνενώνονται σε έναν ενοποιημένο πίνακα και προβάλλονται μέσω ενός επιπλέον πίνακα βαρών $W^O$:
\begin{equation}
\text{MultiHead}(Q,K,V) = \text{Concat}(Z_1, Z_2, \dots, Z_h) W^O.
\end{equation}

Κάθε κεφαλή προσοχής παρέχει μια διακριτή προοπτική επεξεργασίας των δεδομένων, ενώ ο συνδυασμός τους οδηγεί σε πλουσιότερη και εκφραστικότερη αναπαράσταση της πληροφορίας.

\subsection{Η Αρχιτεκτονική του Μετασχηματιστή}

Η αρχιτεκτονική του μετασχηματιστή αναπτύχθηκε το 2017 από ερευνητική ομάδα της Google και παρουσιάστηκε στη θεμελιώδη εργασία "Attention Is All You Need" \cite{vaswani2017attention}. Η συγκεκριμένη αρχιτεκτονική επέτυχε αποτελέσματα τελευταίας τεχνολογίας σε εργασίες μηχανικής μετάφρασης, αποδεικνύοντας την αποτελεσματικότητα του μηχανισμού προσοχής.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images//chapter2/transformer.png}
    \caption{Η αρχιτεκτονική του μετασχηματιστή όπως παρουσιάστηκε στη δημοσίευση "Attention Is All You Need" \cite{vaswani2017attention}}
    \label{fig:transformer}
\end{figure}

Η αρχιτεκτονική του μετασχηματιστή διακρίνεται σε δύο κύρια τμήματα: τον κωδικοποιητή (encoder) που απεικονίζεται στην αριστερή πλευρά του διαγράμματος, και τον αποκωδικοποιητή (decoder) που παρουσιάζεται στη δεξιά πλευρά.

\paragraph{Ο Κωδικοποιητής του Μετασχηματιστή}

Η διαδικασία της επεξεργασίας αρχίζει από την ακολουθία των tokens ενός κειμένου. Κάθε token αντιστοιχίζεται σε ένα διάνυσμα ενσωμάτωσης μέσω ενός εκπαιδεύσιμου πίνακα token-ενσωματώσεων. Δεδομένου ότι ο μηχανισμός αυτοπροσοχής δεν ενσωματώνει πληροφορία σχετικά με τη θέση του κάθε token, προστίθενται τα διανύσματα κωδικοποίησης θέσης (positional encodings). Το άθροισμα των ενσωματώσεων και των διανυσμάτων θέσης συνιστά την είσοδο του κωδικοποιητή.

Κάθε μπλοκ κωδικοποιητή ενσωματώνει τρία θεμελιώδη υπολογιστικά στοιχεία. Το στρώμα αυτοπροσοχής παράγει από τις ενσωματώσεις τα διανύσματα ερωτήσεων, κλειδιών και τιμών. Μέσω του μηχανισμού αυτοπροσοχής, κάθε token ενημερώνει την αναπαράστασή του βασιζόμενο στη συνάφειά του με όλα τα υπόλοιπα tokens της ακολουθίας. Το Feed-Forward δίκτυο εφαρμόζεται ανεξάρτητα σε κάθε αναπαράσταση, υλοποιώντας τη ιδιότητα καθολικής προσέγγισης, που επιτρέπει την προσέγγιση οποιασδήποτε συνεχούς συνάρτησης με επαρκή ακρίβεια. Οι υπολειμματικές συνδέσεις και η κανονικοποίηση στρώματος εφαρμόζονται μετά την προσοχή και το Feed-Forward, βελτιώνοντας τη σταθερότητα της εκπαίδευσης σύμφωνα με την ανάλυση των Kaiming He et al. \cite{he2016deep}.

Η στοίβα $N$ τέτοιων κωδικοποιητών δημιουργεί διαδοχικά εμπλουτισμένες αναπαραστάσεις, καθώς κάθε επίπεδο επεξεργάζεται και βελτιώνει τα αποτελέσματα του προηγούμενου.

\paragraph{Ο Αποκωδικοποιητής του Μετασχηματιστή}

Ο αποκωδικοποιητής αναλαμβάνει την αυτοπαλίνδρομη (autoregressive) διαδικασία παραγωγής της ακολουθίας εξόδου. Σε αντίθεση με τον κωδικοποιητή που επεξεργάζεται ολόκληρη την ακολουθία εισόδου ταυτόχρονα, ο αποκωδικοποιητής λειτουργεί διαδοχικά, παράγοντας κάθε token με βάση τα token που παράχθηκαν προηγουμένως.

Κάθε μπλοκ αποκωδικοποιητή ενσωματώνει τέσσερα θεμελιώδη υπολογιστικά στοιχεία. Το στρώμα μασκαρισμένης αυτοπροσοχής επιτρέπει σε κάθε token της ακολουθίας εξόδου να αλληλεπιδρά αποκλειστικά με τα token που αποκωδικοποιήθηκαν προηγουμένως. Η μάσκα εξασφαλίζει ότι η ροή της πληροφορίας ακολουθεί αιτιακή (causal) κατεύθυνση, δηλαδή κάθε token μπορεί να εξαρτάται μόνο από όσα έχουν ήδη παραχθεί, ποτέ από μελλοντικά. Το στρώμα διασταυρούμενης προσοχής εκτελεί τη σύνδεση μεταξύ της ακολουθίας εισόδου και εξόδου. Οι αναπαραστάσεις από το στρώμα μασκαρισμένης προσοχής χρησιμοποιούνται ως ερωτήσεις, ενώ τα κλειδιά και οι τιμές προέρχονται από τις τελικές αναπαραστάσεις του κωδικοποιητή.

Το Feed-Forward δίκτυο εφαρμόζει μη-γραμμικούς μετασχηματισμούς στις συνδυασμένες αναπαραστάσεις. Οι υπολειμματικές συνδέσεις και η κανονικοποίηση στρώματος εφαρμόζονται μετά από κάθε υπο-στρώμα, διασφαλίζοντας τη σταθερότητα της εκπαίδευσης.

\paragraph{Η Έξοδος του Μετασχηματιστή}

Η τελική έξοδος του μετασχηματιστή περιλαμβάνει ένα γραμμικό στρώμα προβολής που μετασχηματίζει τις αναπαραστάσεις του αποκωδικοποιητή στον χώρο του λεξιλογίου. Το στρώμα αυτό αποτελείται από έναν πίνακα βαρών διαστάσεων $d_{\text{model}} \times |V|$, όπου $|V|$ είναι το μέγεθος του λεξιλογίου. Η εφαρμογή της συνάρτησης softmax (\ref{eq:softmax}) στις προβολές αυτές παράγει κατανομή πιθανοτήτων για κάθε δυνατό επόμενο token. Η διαδικασία αυτή επαναλαμβάνεται αυτοπαλίνδρομα μέχρι την παραγωγή του τελικού token της ακολουθίας εξόδου, επιτρέποντας στο μοντέλο την αυτοπαλίνδρομη παραγωγή κειμένου υψηλής ποιότητας.

% \subsection{Παράμετροι του Μετασχηματιστή}

% Η ανάλυση των εκπαιδεύσιμων παραμέτρων στην αρχιτεκτονική του μετασχηματιστή αποτελεί κρίσιμη πτυχή για την κατανόηση της υπολογιστικής πολυπλοκότητας \cite{amidi2024transformers}. Οι εκπαιδεύσιμες παράμετροι αναφέρονται στα βάρη του νευρωνικού δικτύου που προσαρμόζονται κατά την εκπαίδευση μέσω της διαδικασίας οπισθοδιάδοσης.

% Οι παράμετροι του κωδικοποιητή περιλαμβάνουν τους πίνακες βαρών της πολυκεφαλικής προσοχής, του Feed-Forward δικτύου και της κανονικοποίησης στρώματος. Συγκεκριμένα, κάθε στρώμα κωδικοποιητή απαιτεί τέσσερις πίνακες για την πολυκεφαλική προσοχή ($W^Q, W^K, W^V, W^O$), δύο πίνακες για το Feed-Forward δίκτυο και τέσσερις παραμέτρους κανονικοποίησης ανά διάσταση. Ο συνολικός αριθμός εκπαιδεύσιμων παραμέτρων για $N$ στρώματα κωδικοποιητή υπολογίζεται από τη σχέση:
% \begin{equation}
% N_{\text{encoder}} = N \times \left(4 \times d_{\text{model}} \times d_k \times h + 2 \times d_{\text{model}} \times d_{\text{ff}} + 4 \times d_{\text{model}}\right)
% \end{equation}
% όπου $N$ είναι ο αριθμός στρωμάτων, $d_{\text{model}}$ η διάσταση αναπαράστασης, $d_k$ η διάσταση κλειδιών ανά κεφαλή, $h$ ο αριθμός κεφαλών προσοχής και $d_{\text{ff}}$ η διάσταση του κρυφού στρώματος.

% Οι παράμετροι του αποκωδικοποιητή παρουσιάζουν αυξημένη πολυπλοκότητα λόγω της ύπαρξης δύο διακριτών μηχανισμών προσοχής ανά στρώμα. Πέραν της μασκαρισμένης αυτοπροσοχής, κάθε στρώμα αποκωδικοποιητή ενσωματώνει διασταυρούμενη προσοχή που συνδέει την ακολουθία εισόδου με την ακολουθία εξόδου. Επιπλέον, το Feed-Forward δίκτυο του αποκωδικοποιητή περιλαμβάνει bias παραμέτρους, ενώ η παρουσία τριών στρωμάτων κανονικοποίησης αυξάνει περαιτέρω τον αριθμό παραμέτρων. Ο συνολικός αριθμός εκπαιδεύσιμων παραμέτρων για $N$ στρώματα αποκωδικοποιητή δίνεται από:
% \begin{equation}
% N_{\text{decoder}} = N \times \left(8 \times d_{\text{model}} \times d_k \times h + 2 \times d_{\text{model}} \times d_{\text{ff}} + 6 \times d_{\text{model}} + d_{\text{ff}}\right)
% \end{equation}
% όπου ο παράγοντας 8 αντανακλά τους διπλάσιους πίνακες προσοχής, ενώ οι επιπλέον όροι $6 \times d_{\text{model}} + d_{\text{ff}}$ αφορούν τις τρεις κανονικοποιήσεις και τις bias παραμέτρους αντίστοιχα.

% Το στρώμα εξόδου του μετασχηματιστή αποτελείται από έναν γραμμικό μετασχηματισμό που προβάλλει τις τελικές αναπαραστάσεις του αποκωδικοποιητή στον χώρο του λεξιλογίου. Αυτό περιλαμβάνει έναν πίνακα βαρών διαστάσεων $d_{\text{model}} \times |V|$ και έναν διάνυσμα bias διαστάσεων $|V|$, όπου $|V|$ αντιπροσωπεύει το μέγεθος του λεξιλογίου. Συνεπώς, οι παράμετροι του στρώματος εξόδου υπολογίζονται ως:
% \begin{equation}
% N_{\text{output}} = d_{\text{model}} \times |V| + |V| = |V| \times (d_{\text{model}} + 1)
% \end{equation}

% Επιπρόσθετα, το μοντέλο απαιτεί έναν πίνακα ενσωματώσεων tokens με $|V| \times d_{\text{model}}$ παραμέτρους και πίνακα positional encodings με $L_{\text{max}} \times d_{\text{model}}$ παραμέτρους, όπου $L_{\text{max}}$ αντιστοιχεί στο μέγιστο μήκος ακολουθίας. Η συνολική εξίσωση για την κλασική encoder-decoder αρχιτεκτονική είναι:
% \begin{equation}
% N_{\text{total}} = N_{\text{encoder}} + N_{\text{decoder}} + N_{\text{output}} + N_{\text{embeddings}} + N_{\text{positional}}
% \end{equation}

% \begin{table}[H]
% \centering
% \small
% \begin{tabular}{|l|c|}
% \hline
% \textbf{Συστατικό} & \textbf{Παράμετροι} \\
% \hline
% Κωδικοποιητής ($N$ στρώματα) & $N \times (4d_{\text{model}}d_k h + 2d_{\text{model}}d_{\text{ff}} + 4d_{\text{model}})$ \\
% Αποκωδικοποιητής ($N$ στρώματα) & $N \times (8d_{\text{model}}d_k h + 2d_{\text{model}}d_{\text{ff}} + 6d_{\text{model}} + d_{\text{ff}})$ \\
% Στρώμα εξόδου & $|V| \times (d_{\text{model}} + 1)$ \\
% Token embeddings & $|V| \times d_{\text{model}}$ \\
% Positional encodings & $L_{\text{max}} \times d_{\text{model}}$ \\
% \hline
% \textbf{Σύνολο} & $N_{\text{total}}$ \\
% \hline
% \end{tabular}
% \caption{Συνοπτική κατανομή παραμέτρων αρχιτεκτονικής Transformer}
% \label{tab:transformer-params-summary}
% \end{table}

% Η παραπάνω ανάλυση αποκαλύπτει ότι ο αποκωδικοποιητής συνεισφέρει σημαντικά περισσότερες παραμέτρους από τον κωδικοποιητή, γεγονός που επεξηγεί την αυξημένη υπολογιστική πολυπλοκότητα των εργασιών παραγωγής κειμένου σε σχέση με τις εργασίες κατανόησης. Επιπλέον, η κλιμάκωση των παραμέτρων ακολουθεί κυρίως τετραγωνική σχέση με τη διάσταση του μοντέλου, γεγονός που εξηγεί την εκθετική αύξηση των υπολογιστικών απαιτήσεων σε μεγάλα γλωσσικά μοντέλα.