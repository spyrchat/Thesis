\section{Διακριτοποίηση σε λεκτικές μονάδες}
\label{section:tokens}

Η διαδικασία της διακριτοποίησης σε λεκτικές μονάδες (tokenization) αποτελεί βασικό στοιχείο στη σύγχρονη επεξεργασία φυσικής γλώσσας και εν γένει στα συστήματα τεχνητής νοημοσύνης που χειρίζονται γλωσσικά δεδομένα. Η θεμελιώδης αυτή διαδικασία εισάγει μια μετατροπή του φυσικού κειμένου σε επεξεργάσιμες ψηφιακές οντότητες, επιτρέποντας στους υπολογιστικούς αλγορίθμους τον χειρισμό και την ανάλυση της γλώσσας μέσω μαθηματικών και στατιστικών μεθόδων.

\subsection{Λεκτική μονάδα}
Ως λεκτική μονάδα (token) ορίζεται μια \textbf{διακριτή μονάδα} κειμένου με \textbf{αυθαίρετα καθορισμένο μέγεθος}. Αυθαίρετα γιατί, ανάλογα με τη στρατηγική διακριτοποίησης, μπορεί να θεωρηθεί ως λεκτική μονάδα μια λέξη, μία υπολέξη, ένας χαρακτήρας ή ακόμη και ένα byte, ανάλογα με την επιθυμητή κοκκομετρία (granularity). Παρακάτω θα παρουσιαστεί η ανάγκη ύπαρξης διαφορετικών αλγορίθμων παραγωγής λεκτικών μονάδων καθώς και η ανάγκη εξισορρόπησης της κοκκομετρίας και του μεγέθους του λεξιλογίου με την πολυπλοκότητα της επεξεργασίας.

\subsection{Διακριτοποίηση σε λεκτικές μονάδες}

Ο όρος διακριτοποίηση σε λεκτικές μονάδες (tokenization) αναφέρεται στην αποσύνθεση του κειμένου σε λεκτικές μονάδες όπου κάθε μία αντιστοιχεί σε μία διακριτή λέξη. Η προσέγγιση αυτή είναι εύκολα κατανοητή, καθώς ευθυγραμμίζεται με τη φυσική ανθρώπινη γλωσσική αντίληψη. Ωστόσο, αυτή η μέθοδος δημιουργεί εκτεταμένα λεξιλόγια και αποτυγχάνει σε εκτός λεξιλογίου (out-of-vocabulary, OOV) περιπτώσεις, περιορίζοντας τη γενίκευση.

\subsubsection{Διακριτοποίηση σε Επίπεδο Υπο-Λέξεων}

Η αποσύνθεση των λέξεων σε μικρότερες μονάδες αποτέλεσε σημείο καμπής στην Επεξεργασία Φυσικής Γλώσσας. Μεθοδολογίες όπως η κωδικοποίηση ζεύγους byte (\textit{Byte Pair Encoding}, BPE) \cite{sennrich2015neural}, το \textit{WordPiece} \cite{schuster2012japanese} και το \textit{SentencePiece} \cite{kudo2018sentencepiece} μειώνουν το λεξιλόγιο, διατηρώντας ωστόσο σημασιολογικά σημαντικές ρίζες. Για παράδειγμα, η φράση ``hazel eyes'' μπορεί να αναλυθεί ως \texttt{[haz]} \texttt{[el]} \texttt{[eyes]}. Η διακριτοποίηση σε επίπεδο υπολέξεων θεωρείται σήμερα ο χρυσός κανόνας σε LLMs, καθώς μειώνει τα σφάλματα εκτός λεξιλογίου και ενισχύει τη γενίκευση.

\subsubsection{Διακριτοποίηση σε Επίπεδο Χαρακτήρων}
Η ανάλυση σε χαρακτήρες \cite{kim2016character} εγγυάται πλήρη κάλυψη του λεξιλογίου με το μικρότερο δυνατό λεξιλόγιο, καθώς κάθε χαρακτήρας γίνεται λεκτική μονάδα. Η φράση ``hazel eyes'' παράγει την ακολουθία \texttt{[h]} \texttt{[a]} \texttt{[z]} \texttt{[e]} \texttt{[l]} \texttt{[ ]} \texttt{[e]} \texttt{[y]} \texttt{[e]} \texttt{[s]}. Ωστόσο, αυτή η προσέγγιση αυξάνει δραματικά το μήκος των ακολουθιών, επιβαρύνοντας την εκπαίδευση και την απόδοση κατά το στάδιο απόφασης (inference).
\subsubsection{Διακριτοποίηση σε Επίπεδο Bytes}
Η διακριτοποίηση σε επίπεδο byte προτάθηκε ευρέως με την ανάπτυξη των GPT-2 μοντέλων, όπου χρησιμοποιήθηκε κωδικοποίηση ζεύγους byte στο επίπεδο των byte (byte-level BPE)\cite{radford2019language}. Κάθε byte (0–255) αποτελεί token, π.χ. η φράση ``hazel eyes'' σε UTF-8 δίνει \texttt{[104]} \texttt{[97]} \texttt{[122]} \texttt{[101]} \texttt{[108]} \texttt{[32]} \texttt{[101]} \texttt{[121]} \texttt{[101]} \texttt{[115]}. Η μέθοδος εξασφαλίζει καθολική συμβατότητα με οποιοδήποτε κείμενο, αλλά οδηγεί σε πολύ μεγαλύτερες ακολουθίες χωρίς εγγυημένη σημασιολογική συνοχή.
\begin{table}[H]
\centering
\small
\begin{tabular}{|p{2.2cm}|p{5cm}|p{5cm}|}
\hline
\textbf{Επίπεδο} & \textbf{Χαρακτηριστικά} & \textbf{Παράδειγμα} \\
\hline
Λέξεων & 
Κάθε λέξη αντιμετωπίζεται ως token.
\newline
\textit{Πλεονεκτήματα:} Εύκολη κατανόηση, ταχύτερη εκτέλεση.
\newline
\textit{Μειονεκτήματα:} Μεγάλο λεξιλόγιο, άγνωστες λέξεις. & 
\fbox{\texttt{hazel}} \fbox{\texttt{eyes}} \\
\hline
Υπο-λέξεων & 
Αποσύνθεση λέξεων σε υπο-μονάδες (BPE, WordPiece).
\newline
\textit{Πλεονεκτήματα:} Μειωμένο λεξιλόγιο, καλύτερος χειρισμός άγνωστων λέξεων.
\newline
\textit{Μειονεκτήματα:} Πιο πολύπλοκη αναπαράσταση. & 
\fbox{\texttt{haz}} \fbox{\texttt{el}} \fbox{\texttt{eyes}} \\
\hline
Χαρακτήρων & 
Κάθε χαρακτήρας αντιμετωπίζεται ως token.
\newline
\textit{Πλεονεκτήματα:} Ελάχιστο λεξιλόγιο, απόλυτη κάλυψη.
\newline
\textit{Μειονεκτήματα:} Πολύ μακρές ακολουθίες, αυξημένη πολυπλοκότητα. & 
\fbox{\texttt{h}} \fbox{\texttt{a}} \fbox{\texttt{z}} \fbox{\texttt{e}} \fbox{\texttt{l}} \fbox{\texttt{ }} \fbox{\texttt{e}} \fbox{\texttt{y}} \fbox{\texttt{e}} \fbox{\texttt{s}} \\
\hline
Bytes & 
Κάθε byte αντιμετωπίζεται ως token.
\newline
\textit{Πλεονεκτήματα:} Σταθερό λεξιλόγιο 256 tokens, πλήρης συμβατότητα.
\newline
\textit{Μειονεκτήματα:} Μέγιστο μήκος ακολουθιών, απώλεια σημασιολογίας. & 
\fbox{\texttt{104}} \fbox{\texttt{97}} \fbox{\texttt{122}} \fbox{\texttt{101}} \fbox{\texttt{108}} \fbox{\texttt{32}} \fbox{\texttt{101}} \fbox{\texttt{121}} \fbox{\texttt{101}} \fbox{\texttt{115}} \\
\hline
\end{tabular}
\caption{Συγκριτική ανάλυση επιπέδων διακριτοποίησης}
\label{tab:tokenization_comparison}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/chapter2/context_length_vs_computational_cost.png}
    \caption{Απαιτήσεις μνήμης του GPT-J σε συνάρτηση με το sequence length \cite{cerebras2021context}.}
    \label{fig:gptj_seq_mem}
\end{figure}

Η επιλογή της μεθόδου tokenization παίζει καθοριστικό ρόλο στη συνολική απόδοση ενός γλωσσικού μοντέλου. 
Από τη μία πλευρά, η χρήση tokenization επιπέδου λέξης μειώνει το μήκος των ακολουθιών, αλλά οδηγεί σε υπερβολικά μεγάλο λεξιλόγιο με υψηλά ποσοστά σφαλμάτων εκτός λεξιλογίου. 
Από την άλλη, η χρήση χαρακτήρων ή bytes εξαλείφει τα προβλήματα εκτός λεξιλογίου, αλλά δημιουργεί ακολουθίες πολύ μεγαλύτερου μήκους. 
Το μήκος αυτό έχει άμεσο αντίκτυπο στο υπολογιστικό κόστος, καθώς οι μηχανισμοί αυτοπροσοχής κλιμακώνονται με πολυπλοκότητα $O(n^2)$ ως προς το μήκος της ακολουθίας $n$ \cite{vaswani2017attention}. 

Το γεγονός αυτό αποτυπώνεται χαρακτηριστικά στο Σχήμα~\ref{fig:gptj_seq_mem}, όπου φαίνεται ότι οι απαιτήσεις μνήμης για το GPT-J αυξάνονται εκθετικά με το μήκος ακολουθίας \cite{cerebras2021context}. 
Επομένως, η επιλογή του tokenizer αποτελεί κρίσιμη εξισορρόπηση μεταξύ \textbf{μεγέθους λεξιλογίου} και \textbf{μήκους ακολουθίας}, επηρεάζοντας τόσο την ικανότητα γενίκευσης όσο και την αποδοτικότητα των μοντέλων.

\subsection{Λεξιλόγιο}

Αφού έχει γίνει κατανοητή η διαδικασία τεμαχισμού του κειμένου, απαιτείται η κατανόηση της έννοιας του λεξιλογίου. Το λεξιλόγιο $V$ είναι ένα ορισμένο σύνολο από λεκτικές μονάδες. Χρησιμοποιείται ως αναφορά για την μετατροπή του κειμένου εισόδου σε λεκτικές μονάδες. Το μέγεθος $|V|$ εξαρτάται από δύο παράγοντες όπως προαναφέρθηκε. 
\begin{itemize}
    \item Την κοκκομετρία (granularity): λεπτή κοκκομετρία (επιπέδου byte) σημαίνει μικρότερο λεξιλόγιο ενώ μεγαλύτερη κοκκομετρία (επιπέδου λέξης) έχει ως αποτέλεσμα μεγαλύτερο λεξιλόγιο.
    \item Αρχικό corpus κειμένων: Για παράδειγμα αν για την εκπαίδευση έγινε χρήση πολυγλωσσικών κειμένων τότε θα χρειαστούν πολύ περισσότερα λεκτικές μονάδες για να αναπαραστήσουν το τελικό λεξιλόγιο συγκριτικά με μονογλωσσικά corpuses.
\end{itemize}

\subsection{Είδη αναλυτών λεκτικών μονάδων}
Ο αναλυτής λεκτικών μονάδων, για ευκολία εφεξής θα χρησιμοποιείται ο όρος tokenizer, χωρίζεται σε δύο βασικές κατηγορίες με βάση την μεθοδολογία κατασκευής του\cite{mielke2021between}. Πιο συγκεκριμένα: 

\subsubsection{Tokenizers βασισμένοι σε κανόνες}

Το πρώτο είδος είναι ο tokenizer βασισμένος σε κανόνες (rule-based tokenizer). Αυτό το είδος tokenizer είναι ιδιαίτερα απλό, καθώς συστηματικά τεμαχίζει το κείμενο σύμφωνα με τους κανόνες που έχουν οριστεί. Ο tokenizer επιπέδου λέξης και ο tokenizer επιπέδου χαρακτήρα είναι δύο πολύ κλασσικά παραδείγματα αυτής της οικογένειας. Η συγκεκριμένη μέθοδος δεν χρειάζεται εκπαίδευση και αυτό αποτελεί σημαντικό πλεονέκτημα. 

Ωστόσο, εμφανίζει έναν θεμελιώδη περιορισμό: οι συγκεκριμένοι tokenizers βασίζονται μόνο στα μοτίβα που υπάρχουν στα δεδομένα και δεν λαμβάνουν καθόλου υπόψη τους τη σημασιολογία της κάθε λέξης. Ως εκ τούτου δεν μπορούν να γενικεύσουν σωστά και τα σφάλματα εκτός λεξιλογίου (out-of-vocabulary, OOV) είναι συχνά.

\subsubsection{Στατιστικοί Tokenizers}

Για τον παραπάνω λόγο τα σύγχρονα γλωσσικά μοντέλα καταφεύγουν σε στατιστικούς tokenizers (learned tokenizers). Το δεύτερο είδος εμφανίζει αρκετά βελτιωμένα αποτελέσματα και πετυχαίνει δύο πολύ σημαντικά πράγματα:
\begin{itemize}
    \item Εκμεταλλεύεται την σημασιολογία των λέξεων, λέξεις όπως \textbf{κολύμπι}, \textbf{κολυμβητήριο}, \textbf{κολυμβητής} έχουν όλες κοινή ρίζα. Ένας αποτελεσματικός tokenizer πρέπει να το λαμβάνει αυτό υπόψη τεμαχίζοντας το κείμενο με τέτοιο τρόπο ώστε να κρατηθεί η κοινή ρίζα, για παράδειγμα η υπολέξη \textbf{[κολυμ]}.
    \item Μειώνει σημαντικά τα σφάλματα εκτός λεξιλογίου. Στο προηγούμενο παράδειγμα αν στο αρχικό corpus κειμένου υπήρχαν οι λέξεις \textbf{κολύμπι}, \textbf{κολυμβητήριο} και δεν υπήρχε η λέξη \textbf{κολυμβητής} και χρησιμοποιούταν rule-based tokenizer, όταν το μοντέλο συναντούσε την πρόταση \textbf{Ο κολυμβητής πήδηξε από το βάθρο}, τότε το μοντέλο δεν θα μπορούσε να μετατρέψει σωστά την πρόταση. 
\end{itemize}

\subsection{Κωδικοποίηση Ζεύγους Byte}

Η κωδικοποίηση ζεύγους byte (Byte Pair Encoding, BPE) είναι ένας από τους πλέον δημοφιλής αλγορίθμους tokenization. Αφορά tokenizers σε επίπεδο υπολέξεων και παραλλαγές αυτού χρησιμοποιούνται από μερικά από τα δημοφιλέστερα Μεγάλα Γλωσσικά Μοντέλα (LLM) όπως το ChatGPT της OpenAI και το Llama της Meta \cite{radford2019language}. Ο συγκεκριμένος αλγόριθμος κατασκευάζει το λεξιλόγιο από τις πιο κοινές οντότητες στο corpus εκπαίδευσης.

Ο αλγόριθμος BPE υιοθετήθηκε από την OpenAI για την εκπαίδευση του GPT και στη συνέχεια εφαρμόστηκε ευρέως σε μοντέλα όπως GPT-2, RoBERTa, BART και DeBERTa \cite{sennrich2015neural}. Η θεμελιώδης ιδέα του BPE έγκειται στην επαναληπτική συγχώνευση των πιο συχνών ζευγών χαρακτήρων ή συμβόλων στο corpus εκπαίδευσης, δημιουργώντας σταδιακά ένα λεξιλόγιο υπο-λέξεων.

\subsubsection{Περιγραφή Αλγορίθμου}

Ο αλγόριθμος BPE εκτελείται σε δύο διακριτά στάδια: την εκπαίδευση (training phase) και την κωδικοποίηση (encoding phase). Κατά το στάδιο της εκπαίδευσης, το σύστημα αναλύει το corpus κειμένου και μαθαίνει τις βέλτιστες στρατηγικές συγχώνευσης χαρακτήρων. Κατά το στάδιο της κωδικοποίησης, εφαρμόζει τις μαθημένες στρατηγικές για την τεμαχισμό νέου κειμένου.

Η διαδικασία ξεκινά με τη δημιουργία ενός αρχικού λεξιλογίου που περιέχει όλους τους μοναδικούς χαρακτήρες που απαντώνται στο corpus. Στη συνέχεια, ο αλγόριθμος επαναληπτικά:

1. Υπολογίζει τη συχνότητα εμφάνισης όλων των δυνατών ζευγών διαδοχικών συμβόλων
2. Επιλέγει το ζεύγος με τη μεγαλύτερη συχνότητα 
3. Συγχωνεύει αυτό το ζεύγος σε ένα νέο σύμβολο
4. Ενημερώνει το λεξιλόγιο προσθέτοντας το νέο σύμβολο
5. Επαναλαμβάνει τη διαδικασία μέχρι να επιτευχθεί το επιθυμητό μέγεθος λεξιλογίου

\begin{algorithm}[H]
\caption{Byte Pair Encoding (BPE)}
\label{alg:bpe-simple}
\begin{algorithmic}[1]
  \Input Σώμα κειμένου $C$, μέγιστο μέγεθος λεξιλογίου $V_{\max}$
  \Output Λεξιλόγιο $V$, λίστα συγχωνεύσεων $M$

  \State $W \gets$ λίστα λέξεων από $C$ (με δείκτη τέλους \texttt{</w>})
  \State $V \gets$ σύνολο όλων των αρχικών \emph{χαρακτήρων} (με \texttt{</w>})
  \State $M \gets [\,]$ \Comment{κενή ακολουθία συγχωνεύσεων}

  \While{$|V| < V_{\max}$}
    \State $pairs \gets$ μετρήσεις συχνότητας \emph{γειτονικών} συμβόλων σε όλες τις λέξεις $W$
    \If{$pairs$ είναι κενό} \textbf{break} \EndIf
    \State $(a,b) \gets \arg\max{(x,y)\in pairs} \; \text{freq}(x,y)$ \Comment{πιο συχνό ζεύγος}
    \State $W \gets$ αντικατάστησε σε κάθε λέξη όλες τις εμφανίσεις του $a\,b$ με το νέο σύμβολο $ab$
    \State $V \gets V \cup \{ab\}$
    \State προσάρτησε $(a,b)$ στο $M$
  \EndWhile

  \State \Return $V, M$
\end{algorithmic}
\end{algorithm}

\subsubsection{Παράδειγμα Εκτέλεσης}

Για την καλύτερη κατανόηση της λειτουργίας του BPE, παρουσιάζεται ένα απλοποιημένο παράδειγμα.\footnote{Το παράδειγμα είναι προσαρμοσμένο από \cite{cerebras2021context}}
Έστω ότι διαθέτουμε το ακόλουθο corpus:

\texttt{["low", "lower", "newest", "widest"]}

\textbf{Βήμα 1: Αρχικοποίηση}
Δημιουργία του αρχικού λεξιλογίου με όλους τους μοναδικούς χαρακτήρες:
\texttt{vocab = \{l, o, w, e, r, n, s, t, i, d\}}

Προσθήκη ειδικού συμβόλου τέλους λέξης (</w>):
\begin{itemize}
\item \texttt{low</w>}
\item \texttt{lower</w>}
\item \texttt{newest</w>}
\item \texttt{widest</w>}
\end{itemize}

\textbf{Βήμα 2: Υπολογισμός συχνοτήτων ζευγών}
\begin{itemize}
\item \texttt{es}: 2 εμφανίσεις (newest, widest)
\item \texttt{st}: 2 εμφανίσεις (newest, widest)
\item \texttt{lo}: 2 εμφανίσεις (low, lower)
\end{itemize}

\textbf{Βήμα 3: Συγχώνευση}
Επιλογή του ζεύγους με μεγαλύτερη συχνότητα (π.χ. "es") και δημιουργία νέου συμβόλου:
\texttt{vocab = vocab ∪ \{es\}}

Η διαδικασία συνεχίζεται επαναληπτικά μέχρι την επίτευξη του επιθυμητού μεγέθους λεξιλογίου.

\subsubsection{Παραλλαγές του Byte Pair Encoding}

Παρότι ο αλγόριθμος BPE \cite{sennrich2015neural} αποτέλεσε σημείο καμπής στη δημιουργία υπο-λέξεων, στη συνέχεια εμφανίστηκαν αρκετές παραλλαγές που βελτίωσαν συγκεκριμένα μειονεκτήματα:

\begin{itemize}
  \item \textbf{WordPiece} \cite{schuster2012japanese}: Εισήχθη αρχικά στη φωνητική αναγνώριση ιαπωνικών και κορεατικών. Σε αντίθεση με το BPE που συγχωνεύει με βάση τη συχνότητα, το WordPiece επιλέγει συγχωνεύσεις με βάση τη μεγιστοποίηση της πιθανότητας (likelihood) σε ένα γλωσσικό μοντέλο. Αποτελεί τον tokenizer των BERT μοντέλων.
  
  \item \textbf{Unigram Language Model} \cite{kudo2018sentencepiece}: Πρόκειται για στοχαστική μέθοδο, όπου ξεκινάμε με ένα μεγάλο αρχικό λεξιλόγιο υπο-λέξεων και στη συνέχεια αφαιρούμε υποψήφια tokens με βάση την πιθανότητα να εξηγήσουν καλύτερα το corpus. Αυτή η μέθοδος υλοποιείται στη βιβλιοθήκη SentencePiece και χρησιμοποιείται από μοντέλα όπως XLNet και T5.
\end{itemize}

Η ύπαρξη διαφορετικών προσεγγίσεων υπογραμμίζει το γεγονός ότι η επιλογή του tokenizer επηρεάζει σημαντικά την απόδοση του εκάστοτε γλωσσικού μοντέλου. Η απόφαση για το ποια μέθοδος θα υιοθετηθεί εξαρτάται από παραμέτρους όπως η γλώσσα, το μέγεθος του corpus και οι απαιτήσεις σε ακρίβεια ή ταχύτητα.