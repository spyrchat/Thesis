\section{Ενσωματώσεις}
\label{section:embeddings}
Από την προηγούμενη ενότητα έγινε κατανοητό πώς το κείμενο τεμαχίζεται σε tokens, μια διαδικασία που αποτελεί το αρχικό στάδιο επεξεργασίας σε κάθε γλωσσικό μοντέλο. Το επόμενο βήμα είναι η απόδοση σε κάθε token μιας διανυσματικής αναπαράστασης. Η αναπαράσταση αυτή, γνωστή ως \textbf{ενσωμάτωση} (\textit{embedding}), αποτυπώνει τις σημασιολογικές και συντακτικές σχέσεις των tokens και λειτουργεί ως η βασική είσοδος σε ένα LLM, καθιστώντας δυνατή την αριθμητική επεξεργασία της γλώσσας \cite{devlin2019bert}.

\subsection{One-hot κωδικοποίηση}
Η ανάλυση ξεκινά από την πιο απλή, αλλά ταυτόχρονα προβληματική μέθοδο: την One-Hot κωδικοποίηση (One-hot Encoding, OHE). Σε αυτήν, κάθε token που βρίσκεται στο λεξιλόγιο $V$ αναπαρίσταται με ένα διάνυσμα μεγέθους $|V|$, στο οποίο όλα τα στοιχεία είναι 0, εκτός από εκείνο που αντιστοιχεί στο συγκεκριμένο token, το οποίο έχει τιμή 1.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images//chapter2/One hot encodings.png}
    \caption{Παράδειγμα One-hot κωδικοποίησης}
    \label{fig:one-hot-encodings}
\end{figure}

Η μέθοδος αυτή παρουσιάζει δύο σημαντικά μειονεκτήματα, και για τον λόγο αυτό δεν χρησιμοποιείται στα σύγχρονα γλωσσικά μοντέλα:
\begin{itemize}
    \item \textbf{Απουσία σημασιολογικής πληροφορίας:} Στον χώρο των ενσωματώσεων, σημασιολογικά παρόμοια tokens θα πρέπει να βρίσκονται κοντά μεταξύ τους. Στην παραπάνω αναπαράσταση, όμως, όλα τα tokens του λεξιλογίου σχηματίζουν μεταξύ τους ορθή γωνία. Ως εκ τούτου, το μοντέλο αδυνατεί να κατανοήσει τη νοηματική συνάφεια μεταξύ παρόμοιων tokens.
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.75\linewidth]{images//chapter2/token_similarity.png}
        \caption{Σύγκριση One-hot κωδικοποίησης με σημασιολογική αναπαράσταση}
        \label{fig:token-similarity}
    \end{figure}

    \item \textbf{Υψηλή διαστατικότητα:} Η διάσταση κάθε ενσωμάτωσης είναι ίση με το μέγεθος του λεξιλογίου ($|V|$). Αυτό συνεπάγεται πολύ υψηλό υπολογιστικό κόστος, καθώς ένα τυπικό λεξιλόγιο έχει μέγεθος της τάξης $10^4 - 10^5$.
\end{itemize}

\subsection{Συνεχείς Ενσωματώσεις}
Οι μέθοδοι που παράγουν συνεχείς ενσωματώσεις δημιουργήθηκαν από την ανάγκη αποτύπωσης της σημασιολογίας κάθε λέξης στην αντίστοιχη ενσωμάτωση. Παρακάτω παρουσιάζονται συνοπτικά οι μέθοδοι αναπαράστασης, που οδήγησαν στον μηχανισμό αυτοπροσοχής, ο οποίος χρησιμοποιείται από τα σύγχρονα LLM.
\subsubsection{Word2Vec}
Μια από τις πρώτες μεθόδους για τη δημιουργία συνεχών ενσωματώσεων ήταν το Word2Vec \cite{mikolov2013efficient}. 
Η βασική του ιδέα είναι ότι οι λέξεις που εμφανίζονται σε παρόμοια συμφραζόμενα τείνουν να έχουν παρόμοιες σημασίες, 
και επομένως μπορούν να αναπαρασταθούν με κοντινά διανύσματα σε έναν πολυδιάστατο χώρο.

Το Word2Vec προτάθηκε σε δύο παραλλαγές:
\begin{itemize}
    \item \textbf{Continuous Bag-of-Words (CBOW):} Το μοντέλο προβλέπει τη λέξη-στόχο με βάση τα γειτονικά της tokens. 
    Για παράδειγμα, από τα συμφραζόμενα \textit{``the cat \_ on the mat''}, το μοντέλο μαθαίνει να προβλέπει τη λέξη \textit{``sits''}.
    \item \textbf{Skip-gram:} Η αντίστροφη διαδικασία, όπου από μία λέξη-στόχο το μοντέλο προβλέπει τα γειτονικά της tokens.
\end{itemize}

Και στις δύο περιπτώσεις, η εκπαίδευση γίνεται με έναν απλό νευρωνικό ταξινομητή ενός κρυφού επιπέδου, 
ο οποίος, μετά από πολλές επαναλήψεις, μαθαίνει βάρη που λειτουργούν ως διανυσματικές αναπαραστάσεις των λέξεων. 
Έτσι, λέξεις με παρόμοια χρήση αποκτούν παρόμοια embeddings. 
Ένα κλασικό παράδειγμα αυτής της ιδιότητας είναι ότι τα διανύσματα ικανοποιούν σχέσεις αναλογιών, όπως:
\[
    \text{king} - \text{man} + \text{woman} \approx \text{queen}.
\]

Σε αντίθεση με τη one-hot κωδικοποίηση, το Word2Vec αποτυπώνει σημασιολογικές σχέσεις και οδηγεί σε πολύ πιο χρήσιμες 
αναπαραστάσεις. Ωστόσο, εξακολουθεί να παράγει \textit{στατικά} embeddings: κάθε λέξη έχει μία και μοναδική διανυσματική 
αναπαράσταση, ανεξάρτητα από τα συμφραζόμενα μέσα στα οποία εμφανίζεται.
\subsubsection{Αναδρομικά Νευρωνικά Δίκτυα (RNNs)}
Μετά τις στατικές ενσωματώσεις όπως το Word2Vec, εμφανίστηκαν τα \textbf{αναδρομικά νευρωνικά δίκτυα} (RNNs) 
ως μια λύση για την επεξεργασία ακολουθιακών δεδομένων. Η βασική ιδέα ήταν ότι κάθε κρυφή κατάσταση $h_t$ 
ενημερώνεται με βάση την προηγούμενη κατάσταση $h_{t-1}$ και την τρέχουσα είσοδο $x_t$:
\begin{equation}
    h_t = f(Wx_t + Uh_{t-1}),
\end{equation}
όπου $W, U$ είναι βάρη που μαθαίνονται και $f$ μια μη γραμμική συνάρτηση ενεργοποίησης όπως η tanh \cite{elman1990finding}.
Με αυτόν τον τρόπο, το δίκτυο μπορεί θεωρητικά να «θυμάται» πληροφορίες από το παρελθόν.

Ωστόσο, κατά την εκπαίδευση μέσω οπισθοδιάδοσης στο χρόνο (\textit{backpropagation through time}), 
οι κλίσεις (gradients) είτε εξαφανίζονται είτε εκρήγνυνται, καθιστώντας δύσκολη την εκμάθηση μακροπρόθεσμων εξαρτήσεων. 
Έτσι, τα απλά RNNs είναι αποτελεσματικά μόνο για βραχυπρόθεσμα συμφραζόμενα.

\subsubsection{Long Short-Term Memory (LSTM) και Gated Recurrent Unit (GRU)}
Για να ξεπεραστούν οι αδυναμίες των RNNs, αναπτύχθηκαν πιο σύνθετες μονάδες, όπως τα \textbf{LSTMs} \cite{hochreiter1997long} 
και οι \textbf{GRUs} \cite{cho2014learning}. Τα LSTMs εισάγουν μια κυψέλη μνήμης και μηχανισμούς «πυλών» 
(\textit{gates}) που ρυθμίζουν ποια πληροφορία αποθηκεύεται, ποια ξεχνιέται και ποια εξάγεται σε κάθε χρονικό βήμα. 
Οι GRUs αποτελούν μια απλούστερη παραλλαγή με λιγότερες πύλες, αλλά παρόμοια αποτελεσματικότητα. 
Παρά τη σημαντική βελτίωση, τα μοντέλα αυτά παραμένουν εγγενώς σειριακά: η επεξεργασία κάθε στοιχείου εξαρτάται από το προηγούμενο. 
Αυτό περιορίζει δραστικά την παραλληλοποίηση και καθιστά αργή την εκπαίδευση σε μεγάλα σύνολα δεδομένων. 
Επιπλέον, αν και οι LSTMs μπορούν να διατηρήσουν πληροφορίες σε μεγαλύτερο χρονικό βάθος, 
στην πράξη το εύρος μνήμης παραμένει περιορισμένο.

\subsection{Σημασιολογικές Ενσωματώσεις}
Οι παραδοσιακές στατικές ενσωματώσεις, όπως η Word2Vec, αποτυπώνουν γενικές σημασιολογικές σχέσεις, αλλά αποδίδουν στην ίδια λέξη την ίδια αναπαράσταση ανεξαρτήτως συμφραζομένων. Για παράδειγμα, η λέξη \textit{bank} θα έχει την ίδια ενσωμάτωση τόσο στη φράση \textit{river bank} όσο και στη φράση \textit{bank account}, παρότι η σημασία της είναι εντελώς διαφορετική.

Για να ξεπεραστεί αυτός ο θεμελιώδης περιορισμός, τα σύγχρονα γλωσσικά μοντέλα χρησιμοποιούν τον μηχανισμό προσοχής (attention) για να παράγουν \textbf{σημασιολογικές ενσωματώσεις} (\textit{contextualized embeddings}). Αυτός ο μηχανισμός επιτρέπει σε κάθε token να αναπροσαρμόζει την αναπαράστασή του λαμβάνοντας υπόψη όλα τα υπόλοιπα tokens της ακολουθίας και το συγκεκριμένο πλαίσιο στο οποίο εμφανίζεται.

Η διαδικασία της αυτοπροσοχής (\textit{self-attention}) λειτουργεί ως εξής: για κάθε token $i$ σε μια ακολουθία μήκους $T$, δημιουργούνται τρία διανύσματα μέσω γραμμικών μετασχηματισμών της αρχικής ενσωμάτωσης: το διάνυσμα ερώτησης (\textit{query}) $q_i$, το διάνυσμα κλειδιού (\textit{key}) $k_i$ και το διάνυσμα τιμής (\textit{value}) $v_i$. Τα διανύσματα αυτά υπολογίζονται για όλες τις ενσωματώσεις της ακολουθίας.

Ο βαθμός προσοχής μεταξύ του token $i$ και κάθε άλλου token $j$ στην ακολουθία υπολογίζεται με εσωτερικό γινόμενο των αντίστοιχων διανυσμάτων ερώτησης και κλειδιού, ακολουθούμενο από κανονικοποίηση με softmax (\ref{eq:softmax}):
\begin{equation}
\alpha_{i,j} = \frac{\exp\left(\frac{q_i \cdot k_j}{\sqrt{d_k}}\right)}
{\sum_{l=1}^{T}\exp\left(\frac{q_i \cdot k_l}{\sqrt{d_k}}\right)}
\end{equation}

όπου $d_k$ είναι η διάσταση των διανυσμάτων κλειδιού και χρησιμοποιείται για λόγους αριθμητικής σταθερότητας.

Η νέα σημασιολογική ενσωμάτωση του token $i$ προκύπτει ως σταθμισμένος μέσος όλων των διανυσμάτων τιμής, όπου τα βάρη καθορίζονται από τους βαθμούς προσοχής:
\begin{equation}
z_i = \sum_{j=1}^{T} \alpha_{i,j} v_j
\end{equation}
