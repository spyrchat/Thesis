\section{Θεμελιώδεις έννοιες Νευρωνικών Δικτύων}
\label{section:neural_foundations}

Η κατανόηση των νευρωνικών δικτύων αποτελεί απαραίτητη προϋπόθεση για τη μελέτη των σύγχρονων μοντέλων επεξεργασίας φυσικής γλώσσας. Οι θεμελιώδεις έννοιες που παρουσιάζονται στο κεφάλαιο αυτό, από τον μεμονωμένο τεχνητό νευρώνα έως τη δομή και τη διαδικασία εκπαίδευσης των πολυστρωματικών δικτύων, διαμορφώνουν το εννοιολογικό πλαίσιο που επιτρέπει την κατανόηση προηγμένων αρχιτεκτονικών, όπως οι μηχανισμοί προσοχής (attention mechanisms) και τα μοντέλα μετασχηματιστών που αποτελούν τον πυρήνα των σύγχρονων συστημάτων γλωσσικής μοντελοποίησης. Η ανάλυση ξεκινά από τη βασική υπολογιστική μονάδα, τον τεχνητό νευρώνα, συνεχίζει με τις συναρτήσεις ενεργοποίησης και τη δομή του Πολυστρωματικού Perceptron (MLP), και ολοκληρώνεται με τη διαδικασία μάθησης και τους περιορισμούς που ανακύπτουν κατά την εκπαίδευση βαθιών δικτύων.

\subsection{Τεχνητός Νευρώνας και Βασικές Αρχές}
Ο τεχνητός νευρώνας αποτελεί την στοιχειώδη υπολογιστική μονάδα των νευρωνικών δικτύων. Λαμβάνει εισόδους $x_1,\ldots,x_n$, καθεμία σταθμισμένη με συντελεστή βάρους $w_i$. Το γραμμικό άθροισμα των σταθμισμένων εισόδων και ο όρος πόλωσης $b$ (bias term) γράφονται συνοπτικά ως
\begin{equation}
z = \sum_{i=1}^n w_i x_i + b = w^\top x + b,
\end{equation}
όπου $w$ και $x$ παριστάνουν τα διανύσματα των βαρών και των εισόδων αντίστοιχα. Η τελική έξοδος του νευρώνα προκύπτει εφαρμόζοντας μια συνάρτηση ενεργοποίησης $\sigma(\cdot)$ στο γραμμικό συνδυασμό, δηλαδή
\begin{equation}
h = \sigma(z).
\end{equation}
Η διασύνδεση πολλαπλών τέτοιων μονάδων σε διαδοχικά επίπεδα σχηματίζει ένα νευρωνικό δίκτυο. Το επίπεδο εισόδου δέχεται τα αρχικά δεδομένα, τα ενδιάμεσα κρυφά επίπεδα (hidden layers) υλοποιούν διαδοχικούς μετασχηματισμούς που εξάγουν ιεραρχικές αναπαραστάσεις των δεδομένων, και το επίπεδο εξόδου παράγει την τελική πρόβλεψη.

\subsection{Συναρτήσεις Ενεργοποίησης και ο Ρόλος τους}
Η εισαγωγή μη γραμμικότητας μέσω των συναρτήσεων ενεργοποίησης αποτελεί κρίσιμο στοιχείο για την εκφραστική ισχύ των νευρωνικών δικτύων. Εάν δεν χρησιμοποιηθούν μη γραμμικές συναρτήσεις, η σύνθεση γραμμικών μετασχηματισμών παραμένει γραμμική. Συγκεκριμένα, για δύο διαδοχικά επίπεδα με γραμμικές μόνο πράξεις, έχουμε
\begin{equation}
h^{(2)} = W^{(2)}\!\bigl(W^{(1)}x + b^{(1)}\bigr) + b^{(2)} \;=\; W_{\mathrm{eq}}x + b_{\mathrm{eq}},
\end{equation}
όπου $W_{\mathrm{eq}} = W^{(2)}W^{(1)}$ και $b_{\mathrm{eq}} = W^{(2)}b^{(1)} + b^{(2)}$. Συνεπώς, ελλείψει μη γραμμικής συνάρτησης ενεργοποίησης, το δίκτυο παραμένει αδύνατον να αναπαραστήσει μη γραμμικά διαχωρίσιμα προβλήματα. 

Οι πλέον διαδεδομένες συναρτήσεις ενεργοποίησης περιλαμβάνουν τη σιγμοειδή( \emph{sigmoid}), η οποία ορίζεται ως
\begin{equation}
\sigma(x) = \frac{1}{1 + e^{-x}}, \qquad \sigma'(x) = \sigma(x)\bigl(1-\sigma(x)\bigr),
\end{equation}
και χαρτογραφεί την είσοδο στο διάστημα $(0,1)$, καθιστώντας την κατάλληλη για προβλήματα δυαδικής ταξινόμησης. Η \emph{υπερβολική εφαπτομένη} (hyperbolic tangent) δίνεται από τη σχέση
\begin{equation}
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}, \qquad \tanh'(x) = 1 - \tanh^2(x),
\end{equation}
και χαρτογραφεί την είσοδο στο διάστημα $(-1,1)$, παρέχοντας συμμετρική μη γραμμικότητα γύρω από το μηδέν. Μια σύγχρονη και ευρέως χρησιμοποιούμενη επιλογή είναι η \emph{Rectified Linear Unit (ReLU)},
\begin{equation}
\mathrm{ReLU}(x) = \max(0,x),
\end{equation}
η οποία είναι υπολογιστικά λιτή, εφαρμόζεται με απλή σύγκριση και κατωφλίωση, και μετριάζει σημαντικά το πρόβλημα της εξαφάνισης των κλίσεων για θετικές τιμές εισόδου, καθώς η παράγωγός της είναι σταθερή και ίση με τη μονάδα στην θετική περιοχή. Παρά τα πλεονεκτήματά της, η ReLU μπορεί να οδηγήσει στο φαινόμενο των «νεκρών νευρώνων» (dying ReLU), όπου νευρώνες παύουν να ενεργοποιούνται για οποιαδήποτε είσοδο λόγω της μηδενικής κλίσης στην αρνητική περιοχή, ζήτημα που έχει οδηγήσει στην ανάπτυξη παραλλαγών όπως η Leaky ReLU και η Parametric ReLU.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{images/chapter2/simple_activation_functions.png}
  \caption{Τυπικές συναρτήσεις ενεργοποίησης και οι αντίστοιχες παράγωγοί τους.}
  \label{fig:activation-functions}
\end{figure}

\subsubsection{Η συνάρτηση \texorpdfstring{softmax}{softmax}}
Για προβλήματα πολυταξινόμησης, όπου απαιτείται η ανάθεση κάθε εισόδου σε μία από πολλές διακριτές κατηγορίες, χρησιμοποιείται η συνάρτηση softmax. Δοθέντος ενός διανύσματος ετικέτας (logits) $z\in\mathbb{R}^K$, η softmax ορίζει μια κατανομή πιθανότητας επί των $K$ κλάσεων ως
\begin{equation}
\mathrm{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}, \qquad i=1,\ldots,K,
\label{eq:softmax}
\end{equation}
εξασφαλίζοντας ότι $\sum_{i=1}^{K}\mathrm{softmax}(z_i)=1$ και $\mathrm{softmax}(z_i)\in(0,1)$ για κάθε $i$. Η softmax συνδυάζεται φυσικά με τη συνάρτηση απώλειας διασταυρωμένης εντροπίας (cross-entropy loss) για την εκπαίδευση μοντέλων πολυταξινόμησης. Για έναν στόχο $y$ κωδικοποιημένο ως one-hot διάνυσμα (δηλαδή $y_c=1$ για τη σωστή κλάση $c$ και $y_i=0$ για $i\neq c$), η απώλεια διασταυρωμένης εντροπίας γράφεται
\begin{equation}
L_{\mathrm{CE}} = -\sum_{i=1}^{K} y_i \log\!\bigl(\mathrm{softmax}(z_i)\bigr) = -\log\!\bigl(\mathrm{softmax}(z_c)\bigr),
\end{equation}
όπου η τελευταία ισότητα προκύπτει από το γεγονός ότι μόνο ο όρος της σωστής κλάσης συνεισφέρει στο άθροισμα. Ένα σημαντικό πλεονέκτημα αυτής της σύζευξης είναι ότι η παράγωγος της απώλειας ως προς τις ετικέτες απλοποιείται σημαντικά: εφαρμόζοντας τον κανόνα της αλυσίδας και αξιοποιώντας τις μαθηματικές ιδιότητες της σύνθεσης, προκύπτει $\partial L/\partial z = \hat{y} - y$, όπου $\hat{y}=\mathrm{softmax}(z)$. Αυτή η κομψή μορφή της κλίσης καθιστά την εκπαίδευση ιδιαίτερα αποδοτική και αριθμητικά σταθερή.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{images/chapter2/softmax-example_orig.png}
  \caption{Εποπτική αναπαράσταση της softmax ως χαρτογράφηση από $\mathbb{R}^K$ στο simplex πιθανοτήτων. Πηγή: \cite{mriquestions_softmax}.}
  \label{fig:softmax-schematic}
\end{figure}

\subsection{Το Πολυστρωματικό Perceptron (MLP)}
\label{subsec:mlp}

Αφού έγινε αντιληπτή η λειτουργία των μεμονωμένων νευρώνων και των συναρτήσεων ενεργοποίησης, δύναται να εξεταστεί η οργανωμένη διασύνδεσή τους σε επίπεδα που δημιουργεί δίκτυα με σημαντικά αυξημένη υπολογιστική και αναπαραστασιακή ισχύ. Το Πολυστρωματικό Perceptron αποτελεί τη θεμελιώδη αρχιτεκτονική που επιτρέπει την εκμάθηση πολύπλοκων μη γραμμικών απεικονίσεων.

\paragraph{Ορισμός και ορολογία.}
Ένα \emph{Πολυστρωματικό Perceptron} (Multi\-Layer Perceptron, MLP) είναι ένα \emph{προωθητικής ροής} (feed\-forward) νευρωνικό δίκτυο: η πληροφορία ρέει μονοδρομικά από την είσοδο προς την έξοδο, χωρίς αναδράσεις ή εσωτερική μνήμη κατάστασης. Αυτή η μονοκατευθυντική ροή διαφοροποιεί τα MLP από τα αναδρομικά δίκτυα (Recurrent Neural Networks), τα οποία διαθέτουν κυκλικές συνδέσεις και εσωτερική μνήμη. Το MLP αποτελείται από διαδοχικά \emph{πλήρως συνδεδεμένα} (fully connected ή dense) επίπεδα που υλοποιούν εναλλάξ γραμμικούς και μη γραμμικούς μετασχηματισμούς.

\paragraph{Μαθηματικό μοντέλο ανά επίπεδο.}
Για είσοδο \(h^{(0)}\equiv x\in\mathbb{R}^{n_0}\) και επίπεδα \(l=1,\ldots,L\), κάθε επίπεδο εφαρμόζει τη μετασχηματιστική σχέση
\begin{equation}
z^{(l)} \;=\; W^{(l)} h^{(l-1)} + b^{(l)}, 
\qquad
h^{(l)} \;=\; \sigma^{(l)}\!\bigl(z^{(l)}\bigr),
\label{eq:mlp-layer}
\end{equation}
όπου \(W^{(l)}\in\mathbb{R}^{n_l\times n_{l-1}}\) είναι ο πίνακας βαρών, \(b^{(l)}\in\mathbb{R}^{n_l}\) το διάνυσμα πόλωσης, και \(\sigma^{(l)}:\mathbb{R}\to\mathbb{R}\) μια \emph{συνάρτηση ενεργοποίησης} που εφαρμόζεται κατά στοιχείο (elementwise), δηλαδή \(\sigma^{(l)}(z)=[\sigma^{(l)}(z_1),\ldots,\sigma^{(l)}(z_{n_l})]^\top\). Η \(\sigma\) εισάγει την αναγκαία μη γραμμικότητα που εμποδίζει τη σύνθεση των στρώσεων να καταρρεύσει σε έναν ενιαίο γραμμικό μετασχηματισμό, όπως αναλύθηκε προηγουμένως.

Στα κρυφά επίπεδα, συνηθέστερες επιλογές για \(\sigma\) αποτελούν οι ReLU και οι σύγχρονες παραλλαγές της όπως η GELU (Gaussian Error Linear Unit), λόγω της απλότητας της παραγώγου τους και της καλής αριθμητικής συμπεριφοράς κατά την εκπαίδευση. Στο επίπεδο εξόδου, η επιλογή της συνάρτησης ενεργοποίησης εξαρτάται από το πρόβλημα: για πολυταξινόμηση χρησιμοποιείται η \(\mathrm{softmax}\), για δυαδική ταξινόμηση η \(\mathrm{sigmoid}\), ενώ για προβλήματα παλινδρόμησης συνήθως χρησιμοποιείται η γραμμική ταυτότητα (δηλαδή καμία μη γραμμικότητα).

\paragraph{Διανυσματική μορφή και πολυπλοκότητα.}
Συμβολικά, ένα MLP ορίζει μια παραμετρική συνάρτηση
\[
f_\theta:\mathbb{R}^{n_0}\to\mathbb{R}^{n_L}, 
\qquad
\theta=\{(W^{(l)},b^{(l)})\}_{l=1}^L,
\]
μέσω της επαναληπτικής εφαρμογής της σχέσης \eqref{eq:mlp-layer}. Η συνολική \emph{πολυπλοκότητα παραμέτρων} είναι \(\sum_{l=1}^L (n_l n_{l-1}+n_l)\), όπου \( n_l \) είναι ο αριθμός νευρώνων στο επίπεδο \( l \) και \( n_{l-1} \) στο προηγούμενο επίπεδο. Ο πρώτος όρος \( n_l n_{l-1} \) προέρχεται από τα βάρη μεταξύ των επιπέδων, ενώ ο δεύτερος όρος \( n_l \) από τους αντίστοιχους bias όρους. Αυτή η έκφραση αποτυπώνει τον βασικό συμβιβασμό μεταξύ \emph{πλάτους} και \emph{βάθους} της αρχιτεκτονικής: η αύξηση του πλάτους (δηλαδή του αριθμού νευρώνων ανά επίπεδο) αυξάνει άμεσα την εκφραστικότητα αλλά με τετραγωνικό κόστος παραμέτρων, ενώ το πρόσθετο βάθος (δηλαδή περισσότερα επίπεδα) επιτρέπει ιεραρχικούς, συνθετικούς μετασχηματισμούς με γραμμική αύξηση παραμέτρων ανά επίπεδο. Αυτή η παρατήρηση έχει σημαντικές πρακτικές συνέπειες: βαθύτερα δίκτυα τείνουν να επιτυγχάνουν υψηλότερη απόδοση με λιγότερες παραμέτρους σε σύγκριση με ρηχά αλλά πλατιά δίκτυα, υπό την προϋπόθεση ότι η εκπαίδευσή τους παραμένει σταθερή.

\paragraph{Εκφραστικότητα και Θεώρημα Καθολικής Προσέγγισης.}
Το θεώρημα καθολικής προσέγγισης (universal approximation theorem) \cite{hornik1989multilayer} εγγυάται ότι ένα MLP με τουλάχιστον ένα κρυφό επίπεδο επαρκούς πλάτους και \emph{μη πολυωνυμική, μη γραμμική} ενεργοποίηση μπορεί να προσεγγίσει αυθαίρετα καλά κάθε συνεχή συνάρτηση σε συμπαγή υποσύνολα του $\mathbb{R}^n$. Το θεώρημα αυτό αποτελεί θεμελιώδες αποτέλεσμα για την κατανόηση της θεωρητικής δυναμικής των νευρωνικών δικτύων, καθώς διασφαλίζει την \emph{εκφραστικότητα} του μοντέλου, δηλαδή τι \emph{μπορεί} να αναπαρασταθεί δοθέντων επαρκών πόρων.

Ωστόσο, είναι κρίσιμο να κατανοηθεί ότι το θεώρημα δεν παρέχει εγγυήσεις σχετικά με την \emph{εκμάθηση}: δεν εγγυάται ότι οι αλγόριθμοι βελτιστοποίησης θα εντοπίσουν στην πράξη τις κατάλληλες παραμέτρους, ούτε ότι ο απαιτούμενος αριθμός νευρώνων θα είναι πρακτικά εύλογος. Στα βαθιά δίκτυα με ReLU ενεργοποίηση, το μοντέλο ορίζει \emph{κομματικά γραμμικές} συναρτήσεις: το βάθος αυξάνει εκθετικά τον αριθμό των διακριτών περιοχών γραμμικότητας, βελτιώνοντας την ικανότητα μοντελοποίησης πολύπλοκων αποφασιστικών ορίων με σχετική οικονομία παραμέτρων. Αυτή η ιδιότητα εξηγεί εν μέρει γιατί τα σύγχρονα βαθιά δίκτυα έχουν επιτύχει τόσο εντυπωσιακά αποτελέσματα σε πρακτικές εφαρμογές.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{images/chapter2/nn.png}
  \caption{Αρχιτεκτονική Πολυστρωματικού Perceptron με δύο κρυφά επίπεδα (feed\-forward).}
  \label{fig:mlp}
\end{figure}

\subsection{Η Διαδικασία Μάθησης στα Νευρωνικά Δίκτυα}
Η εκπαίδευση ενός νευρωνικού δικτύου αποτελεί επαναληπτική διαδικασία προσαρμογής των παραμέτρων με στόχο την ελαχιστοποίηση μιας συνάρτησης απώλειας που ποσοτικοποιεί την απόκλιση μεταξύ των προβλέψεων του μοντέλου και των επιθυμητών εξόδων. Κάθε κύκλος εκπαίδευσης περιλαμβάνει τρία βασικά στάδια: την πρόδρομη διάδοση (forward propagation) για την παραγωγή πρόβλεψης, τον υπολογισμό της απώλειας μέσω σύγκρισης με την πραγματική έξοδο, την οπισθοδιάδοση (backpropagation) για τον αποδοτικό υπολογισμό των κλίσεων, και την ενημέρωση των παραμέτρων σύμφωνα με έναν κανόνα βελτιστοποίησης. Η διαδικασία επαναλαμβάνεται επί πολλαπλών κύκλων (epochs) διέλευσης του συνόλου εκπαίδευσης.

\subsubsection{Το πρόβλημα της εκπαίδευσης ως βελτιστοποίηση}
Δοθέντος συνόλου δεδομένων εκπαίδευσης $\mathcal{D}=\{(x_i,y_i)\}_{i=1}^N$, όπου $x_i$ αναπαριστά την είσοδο και $y_i$ την αντίστοιχη επιθυμητή έξοδο, επιδιώκεται η εύρεση παραμέτρων $\theta$ που ελαχιστοποιούν την εμπειρική συνάρτηση κινδύνου (empirical risk function) \cite{goodfellow2016deep}:
\begin{equation}
J(\theta) = \frac{1}{N} \sum_{i=1}^N L\!\bigl(f_\theta(x_i),y_i\bigr) \;+\; \lambda\, R(\theta),
\end{equation}
όπου $L$ είναι η συνάρτηση απώλειας που μετρά την απόκλιση μεταξύ πρόβλεψης $f_\theta(x_i)$ και πραγματικής εξόδου $y_i$, ενώ $R(\theta)$ αποτελεί όρο κανονικοποίησης (regularization term) με συντελεστή $\lambda\geq 0$. Ο όρος κανονικοποίησης, συνήθως της μορφής $\ell_2$ (weight decay), δηλαδή $R(\theta)=\|\theta\|_2^2$, περιορίζει την πολυπλοκότητα του μοντέλου και λειτουργεί ως μηχανισμός προστασίας έναντι της υπερπροσαρμογής (overfitting), συμπληρώνοντας άλλες πρακτικές όπως η έγκαιρη διακοπή της εκπαίδευσης (early stopping).

\subsubsection{Επικλινής κάθοδος και στοχαστικές παραλλαγές}
Η \emph{επικλινής κάθοδος} (Gradient Descent, GD) αποτελεί τον κλασικό αλγόριθμο βελτιστοποίησης που ενημερώνει τις παραμέτρους προς την αρνητική κατεύθυνση της κλίσης της συνάρτησης κινδύνου:
\begin{equation}
\theta_{t+1} = \theta_t - \eta \,\nabla_\theta J(\theta_t),
\end{equation}
όπου $\eta>0$ είναι ο \emph{ρυθμός μάθησης} (learning rate), υπερπαράμετρος που ελέγχει το μέγεθος του βήματος ενημέρωσης. Στα σύγχρονα συστήματα βαθιάς μάθησης, η πλήρης επικλινής κάθοδος υποκαθίσταται από τη \emph{στοχαστική επικλινή κάθοδο} (Stochastic Gradient Descent, SGD), κατά την οποία η κλίση εκτιμάται από υποσύνολα δειγμάτων σταθερού μεγέθους, τα οποία ονομάζονται mini-batches. Για ένα mini-batch $\mathcal{B}$ μεγέθους $B\ll N$, η εκτίμηση της κλίσης γράφεται
\[
\nabla_\theta J_{\mathcal{B}}=\frac{1}{B}\sum_{i\in\mathcal{B}}\nabla_\theta L\!\bigl(f_\theta(x_i),y_i\bigr),
\]
πράγμα που μειώνει δραστικά το υπολογιστικό κόστος ανά ενημέρωση και εισάγει ελεγχόμενη στοχαστικότητα στη διαδικασία βελτιστοποίησης. Η στοχαστικότητα αυτή, παρά την πρώτη εντύπωση, μπορεί να είναι ευεργετική: βοηθά το μοντέλο να διαφύγει από ρηχά τοπικά ελάχιστα και να εξερευνήσει ευρύτερες περιοχές του χώρου παραμέτρων. Για επαρκώς μικρά βήματα $\eta$, το ανάπτυγμα Taylor εγγυάται τοπική μείωση του κόστους \cite{bottou2018optimization}.

\subsubsection{Backpropagation: αποδοτικός υπολογισμός κλίσεων}
Ο αλγόριθμος backpropagation \cite{rumelhart1986learning} αποτελεί τον πυρήνα της εκπαίδευσης των νευρωνικών δικτύων. Εφαρμόζει συστηματικά τον κανόνα της αλυσίδας ώστε να υπολογιστούν οι παράγωγοι της συνάρτησης απώλειας ως προς κάθε παράμετρο του δικτύου, με υπολογιστική πολυπλοκότητα που είναι συγκρίσιμη με αυτή της πρόδρομης διάδοσης. Η βασική ιδέα συνίσταται στον αναδρομικό υπολογισμό των ευαισθησιών κάθε επιπέδου, ξεκινώντας από το επίπεδο εξόδου και κινούμενοι προς την είσοδο.

Θέτοντας $z^{(l)}=W^{(l)}h^{(l-1)}+b^{(l)}$ για το προ-ενεργοποίησης διάνυσμα και $h^{(l)}=\sigma^{(l)}(z^{(l)})$ για το διάνυσμα ενεργοποιήσεων του επιπέδου $l$, ορίζουμε τις ευαισθησίες $\delta^{(l)}$ ως την παράγωγο της απώλειας ως προς το $z^{(l)}$. Οι ευαισθησίες υπολογίζονται αναδρομικά ξεκινώντας από το τελικό επίπεδο:
\begin{align}
\delta^{(L)} &= \nabla_{h^{(L)}} L \odot (\sigma^{(L)})'(z^{(L)}), \\
\delta^{(l)} &= (W^{(l+1)})^\top \delta^{(l+1)} \odot (\sigma^{(l)})'(z^{(l)}), \qquad l=L-1,\ldots,1,
\end{align}
όπου το σύμβολο $\odot$ παριστά το γινόμενο κατά στοιχείο (elementwise ή Hadamard product) των διανυσμάτων. Αφού υπολογιστούν οι ευαισθησίες, οι κλίσεις ως προς τις παραμέτρους κάθε επιπέδου προκύπτουν άμεσα ως:
\begin{align}
\frac{\partial J}{\partial W^{(l)}} &= \delta^{(l)} (h^{(l-1)})^\top,\qquad
\frac{\partial J}{\partial b^{(l)}} = \delta^{(l)}.
\end{align}
Η κομψότητα του αλγορίθμου έγκειται στο ότι, με μία πρόδρομη και μία ανάδρομη διάδοση, υπολογίζονται όλες οι απαιτούμενες παράγωγοι, καθιστώντας την εκπαίδευση βαθιών δικτύων πρακτικά εφικτή.

\begin{algorithm}
\caption{Εκπαίδευση με mini\hyp batch στοχαστική επικλινή κάθοδο και backpropagation}
\label{alg:training}
\begin{algorithmic}[1]
\Input Σύνολο εκπαίδευσης $\mathcal{D}$, ρυθμός μάθησης $\eta$, μέγεθος mini\hyp batch $B$, αριθμός εποχών $E$
\Output Βελτιστοποιημένες παράμετροι $\theta^\ast$
\State Αρχικοποίησε $\theta$ \Comment{π.χ. Xavier/Glorot}
\For{$\mathrm{epoch}=1$ \textbf{To} $E$}
  \ForAll{mini\hyp batches $\mathcal{B}$ μεγέθους $B$ από το $\mathcal{D}$}
    \State Υπολόγισε προβλέψεις $\hat{y}_i=f_\theta(x_i)$ για $(x_i,y_i)\in\mathcal{B}$ \Comment{Forward pass}
    \State Υπολόγισε $J_{\mathcal{B}}$ και κλίσεις $g=\nabla_\theta J_{\mathcal{B}}$ μέσω backpropagation
    \State Ενημέρωσε παραμέτρους: $\theta \gets \theta - \eta\, g$
  \EndFor
  \State \textit{Προαιρετικά:} Αξιολόγηση κριτηρίου διακοπής σε σύνολο επικύρωσης
\EndFor
\State \Return $\theta$
\end{algorithmic}
\end{algorithm}

\subsubsection{Σύγχρονες μέθοδοι βελτιστοποίησης και τακτικές εκπαίδευσης}
Πέραν της βασικής επικλινούς καθόδου, στα σύγχρονα συστήματα βαθιάς μάθησης εφαρμόζονται προσαρμοστικές μέθοδοι βελτιστοποίησης που εκτιμούν και αξιοποιούν ροπές των κλίσεων. Ο αλγόριθμος Adam (Adaptive Moment Estimation) \cite{kingma2014adam} αποτελεί ίσως τον πλέον διαδεδομένο αλγόριθμο στην πράξη, καθώς συνδυάζει όρο ορμής και προσαρμοστική κλιμάκωση του ρυθμού μάθησης ανά παράμετρο:
\begin{align}
m_t &= \beta_1 m_{t-1} + (1-\beta_1) g_t,\qquad
v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2, \\
\theta_{t+1} &= \theta_t - \eta \,\frac{m_t/(1-\beta_1^t)}{\sqrt{v_t/(1-\beta_2^t)}+\epsilon},
\end{align}
όπου $g_t=\nabla_\theta J(\theta_t)$ είναι η κλίση στο βήμα $t$, $m_t$ και $v_t$ είναι εκτιμητές της πρώτης και δεύτερης ροπής αντίστοιχα, $\beta_1,\beta_2\in(0,1)$ είναι παράμετροι εκθετικής απόσβεσης (τυπικά $\beta_1=0.9$ και $\beta_2=0.999$), και $\epsilon$ είναι μικρή σταθερά για αριθμητική σταθερότητα. Ο Adam προσαρμόζει το ρυθμό μάθησης για κάθε παράμετρο ξεχωριστά, επιτρέποντας ταχύτερη σύγκλιση και καλύτερη διαχείριση παραμέτρων με διαφορετικές κλίμακες μεγεθών.

Η επιλογή της αρχικοποίησης των βαρών αποτελεί κρίσιμο παράγοντα για τη σταθερότητα και την επιτυχία της εκπαίδευσης. Η αρχικοποίηση Glorot/Xavier \cite{glorot2010understanding} επιλέγει τα αρχικά βάρη από κατανομή με διασπορά που εξαρτάται από τις διαστάσεις εισόδου και εξόδου του επιπέδου, με στόχο τη διατήρηση της διασποράς των ενεργοποιήσεων και των κλίσεων σε παρόμοια επίπεδα καθ' ύψος του δικτύου. Αυτή η ισορροπία είναι ουσιώδης για την αποφυγή προβλημάτων εξαφάνισης ή έκρηξης των κλίσεων στα πρώτα στάδια της εκπαίδευσης.

Ως τεχνικές τακτοποίησης και προστασίας από την υπερπροσαρμογή περιλαμβάνονται η ποινικοποίηση $\ell_2$ των βαρών (weight decay), η τεχνική dropout που απενεργοποιεί τυχαία κλάσματα νευρώνων κατά την εκπαίδευση, και η έγκαιρη διακοπή (early stopping) που τερματίζει την εκπαίδευση όταν η απόδοση στο σύνολο επικύρωσης αρχίσει να υποβαθμίζεται. Αυτές οι μέθοδοι συμβάλλουν συλλογικά στη βελτίωση της ικανότητας γενίκευσης του μοντέλου σε δεδομένα που δεν έχει συναντήσει κατά την εκπαίδευση.

\subsection{Περιορισμοί των Πολυστρωματικών Perceptron}
Παρά την εκφραστική τους ισχύ και την ευρεία χρήση τους, τα Πολυστρωματικά Perceptron εμφανίζουν ορισμένους εγγενείς περιορισμούς που καθιστούν δύσκολη την εφαρμογή τους σε συγκεκριμένες κατηγορίες προβλημάτων.

Πρώτον, η εκπαίδευση πολύ βαθιών MLPs δυσχεραίνεται από τα φαινόμενα των εξαφανιζόμενων και των εκρηκτικών κλίσεων (vanishing and exploding gradients) \cite{bengio1994learning}. Κατά την ανάδρομη διάδοση, η κλίση προς τα πρώιμα επίπεδα του δικτύου προκύπτει από το γινόμενο των κλίσεων όλων των ενδιάμεσων επιπέδων. Όταν οι τοπικές κλίσεις είναι κατά μέσο όρο μικρότερες της μονάδας, το γινόμενο τους τείνει να συρρικνώνεται εκθετικά καθώς το βάθος αυξάνεται, με αποτέλεσμα οι παράμετροι των πρώιμων επιπέδων να εκπαιδεύονται εξαιρετικά αργά ή καθόλου. Αντίθετα, όταν οι τοπικές κλίσεις υπερβαίνουν την μονάδα, το γινόμενο διογκώνεται εκθετικά, προκαλώντας αριθμητική αστάθεια και απόκλιση της βελτιστοποίησης. Αυτά τα προβλήματα περιορίζουν στην πράξη το βάθος των MLPs και ενθάρρυναν την ανάπτυξη εξελιγμένων αρχιτεκτονικών με παράκαμψη συνδέσεων (residual connections), όπως τα ResNets, που μετριάζουν σημαντικά αυτά τα ζητήματα.

Δεύτερον, τα MLPs προϋποθέτουν είσοδο σταθερού μεγέθους. Κάθε επίπεδο έχει προκαθορισμένο αριθμό εισόδων και εξόδων, γεγονός που καθιστά προβληματική την επεξεργασία ακολουθιών μεταβλητού μήκους, όπως προτάσεις φυσικής γλώσσας ή χρονοσειρές.

Τρίτον, λόγω της πλήρους διασύνδεσης των επιπέδων, τα MLPs δεν είναι ικανά να μοντελοποιήσουν αποδοτικά εξαρτήσεις μεγάλης εμβέλειας σε ακολουθιακά ή χωρικά δεδομένα. Για παράδειγμα, σε μια πρόταση, η σημασία μιας λέξης μπορεί να εξαρτάται από λέξεις που βρίσκονται πολλές θέσεις μακριά. Ενώ ένα αρκετά βαθύ MLP θεωρητικά θα μπορούσε να συλλάβει τέτοιες εξαρτήσεις, στην πράξη η εκμάθησή τους απαιτεί τεράστιο αριθμό παραμέτρων και δεδομένων εκπαίδευσης.

Αυτοί οι περιορισμοί οδήγησαν στην ανάπτυξη εξειδικευμένων αρχιτεκτονικών που αντιμετωπίζουν συγκεκριμένα πρoβλήματα: τα Αναδρομικά Νευρωνικά Δίκτυα (Recurrent Neural Networks, RNNs) και οι παραλλαγές τους όπως τα LSTM και GRU εισάγουν εσωτερική κατάσταση για την επεξεργασία ακολουθιών, τα Συνελικτικά Νευρωνικά Δίκτυα (Convolutional Neural Networks, CNNs) αξιοποιούν τοπικές δομές σε χωρικά δεδομένα, και τελικά οι μηχανισμοί προσοχής (attention mechanisms) και τα μοντέλα μετασχηματιστών επιτρέπουν την άμεση μοντελοποίηση εξαρτήσεων αυθαίρετης εμβέλειας σε ακολουθίες, αποτελώντας το θεμέλιο των σύγχρονων μοντέλων επεξεργασίας φυσικής γλώσσας που θα εξεταστούν στα επόμενα κεφάλαια της παρούσας εργασίας.