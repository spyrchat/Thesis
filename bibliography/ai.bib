@article{ji2023survey,
  title   = {Survey of Hallucination in Natural Language Generation},
  author  = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal = {{ACM} Computing Surveys},
  volume  = {55},
  number  = {12},
  pages   = {1--38},
  year    = {2023}
}



@inproceedings{lin2022truthfulqa,
  title     = {Truthful{QA}: Measuring How Models Mimic Human Falsehoods},
  author    = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics ({ACL})},
  pages     = {3214--3252},
  year      = {2022}
}



@article{sharir2020cost,
  title      = {The Cost of Training {NLP} Models: A Concise Overview},
  author     = {Sharir, Or and Peleg, Barak and Shoham, Yoav},
  journal    = {arXiv},
  eprint     = {2004.08900},
  eprinttype = {arXiv},
  year       = {2020}
}

@book{robertson2009probabilistic,
  title     = {The Probabilistic Relevance Framework: {BM25} and Beyond},
  author    = {Robertson, Stephen and Zaragoza, Hugo},
  year      = {2009},
  publisher = {Now Publishers Inc.}
}

@inproceedings{karpukhin2020dense,
  title     = {Dense Passage Retrieval for Open-Domain Question Answering},
  author    = {Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  pages     = {6769--6781},
  year      = {2020}
}

@article{ma2023fine,
  title      = {Fine-Tuning {LLaMA} for Multi-Stage Text Retrieval},
  author     = {Ma, Xueguang and Guo, Liang and Yang, Ruqing and Zhang, Yixing and Lin, Jimmy},
  journal    = {arXiv},
  eprint     = {2310.08319},
  eprinttype = {arXiv},
  year       = {2023}
}

@article{chen2024bge,
  title      = {{BGE} {M3}-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},
  author     = {Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},
  journal    = {arXiv},
  eprint     = {2402.03216},
  eprinttype = {arXiv},
  year       = {2024}
}

@article{gao2023retrieval,
  title      = {Retrieval-Augmented Generation for Large Language Models: A Survey},
  author     = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal    = {arXiv},
  eprint     = {2312.10997},
  eprinttype = {arXiv},
  year       = {2023}
}

@inproceedings{lewis2020retrieval,
  title     = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
  author    = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  booktitle = {Advances in Neural Information Processing Systems ({NeurIPS})},
  volume    = {33},
  pages     = {9459--9474},
  year      = {2020}
}

@inproceedings{borgeaud2022improving,
  title     = {Improving Language Models by Retrieving from Trillions of Tokens},
  author    = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and others},
  booktitle = {International Conference on Machine Learning ({ICML})},
  pages     = {2206--2240},
  year      = {2022}
}

@inproceedings{vaswani2017attention,
  title     = {Attention Is All You Need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems ({NeurIPS})},
  volume    = {30},
  year      = {2017}
}

@misc{radford2018improving,
  title        = {Improving Language Understanding by Generative Pre-Training},
  author       = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year         = {2018},
  howpublished = {{OpenAI} Technical Report}
}

@article{openai2023gpt4,
  title      = {{GPT-4} Technical Report},
  author     = {{OpenAI}},
  journal    = {arXiv},
  eprint     = {2303.08774},
  eprinttype = {arXiv},
  year       = {2023}
}

@article{jiang2023mistral,
  title      = {Mistral 7B},
  author     = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and others},
  journal    = {arXiv},
  eprint     = {2310.06825},
  eprinttype = {arXiv},
  year       = {2023}
}

@inproceedings{brown2020language,
  title     = {Language Models Are Few-Shot Learners},
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D. and Dhariwal, Prafulla and others},
  booktitle = {Advances in Neural Information Processing Systems ({NeurIPS})},
  volume    = {33},
  pages     = {1877--1901},
  year      = {2020}
}

@article{bubeck2023sparks,
  title      = {Sparks of Artificial General Intelligence: Early Experiments with {GPT}-4},
  author     = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and others},
  journal    = {arXiv},
  eprint     = {2303.12712},
  eprinttype = {arXiv},
  year       = {2023}
}

@inproceedings{izacard2021leveraging,
  title     = {Leveraging Passage Retrieval with Generative Models for Open-Domain Question Answering},
  author    = {Izacard, Gautier and Grave, Edouard},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics ({EACL})},
  pages     = {874--880},
  year      = {2021}
}

@article{singhal2023large,
  title   = {Large Language Models Encode Clinical Knowledge},
  author  = {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. Sara and Wei, Jason and Chung, Hyung Won and others},
  journal = {Nature},
  volume  = {620},
  number  = {7972},
  pages   = {172--180},
  year    = {2023}
}

@article{wang2022text,
  title      = {Text Embeddings by Weakly-Supervised Contrastive Pre-Training},
  author     = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  journal    = {arXiv},
  eprint     = {2212.03533},
  eprinttype = {arXiv},
  year       = {2022}
}

@article{xiao2023bge,
  title      = {C-Pack: Packaged Resources to Advance General Chinese Embedding},
  author     = {Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Muennighoff, Niklas},
  journal    = {arXiv},
  eprint     = {2309.07597},
  eprinttype = {arXiv},
  year       = {2023}
}

@article{vu2023freshllms,
  title      = {{FreshLLMs}: Refreshing Large Language Models with Search Engine Augmentation},
  author     = {Vu, Tu and Iyyer, Mohit and Wang, Xuezhi and Constant, Noah and Wei, Jerry and Wei, Jason and others},
  journal    = {arXiv},
  eprint     = {2310.03214},
  eprinttype = {arXiv},
  year       = {2023}
}

@article{kadavath2022language,
  title      = {Language Models (Mostly) Know What They Know},
  author     = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and others},
  journal    = {arXiv},
  eprint     = {2207.05221},
  eprinttype = {arXiv},
  year       = {2022}
}

@inproceedings{mallen2023trust,
  title     = {When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories},
  author    = {Mallen, Alex and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics ({ACL})},
  pages     = {9802--9822},
  year      = {2023}
}
