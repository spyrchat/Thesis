@book{jurafsky2009speech,
  title     = {Speech and Language Processing},
  author    = {Jurafsky, Daniel and Martin, James H.},
  year      = {2009},
  publisher = {Pearson Prentice Hall}
}

@inproceedings{sennrich2015neural,
  title     = {Neural Machine Translation of Rare Words with Subword Units},
  author    = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ({ACL})},
  pages     = {1715--1725},
  year      = {2015}
}

@inproceedings{schuster2012japanese,
  title     = {Japanese and Korean Voice Search},
  author    = {Schuster, Mike and Nakajima, Kaisuke},
  booktitle = {2012 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  pages     = {5149--5152},
  year      = {2012},
  publisher = {IEEE}
}

@inproceedings{kudo2018sentencepiece,
  title     = {SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing},
  author    = {Kudo, Taku},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages     = {66--71},
  year      = {2018}
}

@inproceedings{kim2016character,
  title     = {Character-Aware Neural Language Models},
  author    = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M.},
  booktitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  volume    = {30},
  number    = {1},
  year      = {2016}
}

@article{radford2019language,
  title      = {Language Models are Unsupervised Multitask Learners},
  author     = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal    = {arXiv},
  eprint     = {1901.05207},
  eprinttype = {arXiv},
  year       = {2019},
  note       = {OpenAI Technical Report}
}

@inproceedings{devlin2019bert,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of {NAACL}-{HLT}},
  year      = {2019}
}

@article{mikolov2013efficient,
  title      = {Efficient Estimation of Word Representations in Vector Space},
  author     = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal    = {arXiv},
  eprint     = {1301.3781},
  eprinttype = {arXiv},
  year       = {2013}
}

@article{elman1990finding,
  title     = {Finding Structure in Time},
  author    = {Elman, Jeffrey L.},
  journal   = {Cognitive Science},
  volume    = {14},
  number    = {2},
  pages     = {179--211},
  year      = {1990},
  publisher = {Wiley Online Library}
}

@inproceedings{cho2014learning,
  title     = {Learning Phrase Representations Using {RNN} Encoder-Decoder for Statistical Machine Translation},
  author    = {Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  pages     = {1724--1734},
  year      = {2014}
}

@inproceedings{bahdanau2015neural,
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  author    = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle = {International Conference on Learning Representations ({ICLR})},
  year      = {2015}
}

@article{mielke2021between,
  title      = {Between Words and Characters: A Brief History of Open-Vocabulary Modeling and Tokenization in {NLP}},
  author     = {Mielke, Sabrina J. and Alyafeai, Zaid and Salesky, Elizabeth and Raffel, Colin and Dey, Manan and Gall{\'e}, Matthias and Raja, Arun and Si, Chenglei and Lee, Wilson Y. and Sagot, Beno{\^i}t and others},
  journal    = {arXiv},
  eprint     = {2112.10508},
  eprinttype = {arXiv},
  year       = {2021}
}

@inbook{amidi2024transformersimage,
  title     = {Super Study Guide: Transformers},
  author    = {Amidi, Afshine and Amidi, Shervine},
  year      = {2024},
  pages     =  {70--72},   
  note      = {\textcopyright~2024 Afshine Amidi and Shervine Amidi}
}

@inbook{amidi2024transformers,
  title     = {Super Study Guide: Transformers},
  author    = {Amidi, Afshine and Amidi, Shervine},
  year      = {2024},
  pages     = {83--91},   
  note      = {\textcopyright~2024 Afshine Amidi and Shervine Amidi}
}

@inproceedings{yun2020transformers,
  title     = {Are Transformers Universal Approximators of Sequence-to-Sequence Functions?},
  author    = {Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J. and Kumar, Sanjiv},
  booktitle = {International Conference on Learning Representations ({ICLR})},
  year      = {2020}
}

@inproceedings{clark2019does,
  title     = {What Does {BERT} Look At? An Analysis of {BERT}'s Attention},
  author    = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  booktitle = {Proceedings of the 2019 {ACL} Workshop Blackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}},
  pages     = {276--286},
  year      = {2019}
}

@inproceedings{vig2019analyzing,
  title     = {Analyzing the Structure of Attention in a Transformer Language Model},
  author    = {Vig, Jesse and Belinkov, Yonatan},
  booktitle = {Proceedings of the 2019 {ACL} Workshop Blackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}},
  pages     = {63--76},
  year      = {2019}
}

@article{cybenko1989approximation,
  title     = {Approximation by Superpositions of a Sigmoidal Function},
  author    = {Cybenko, George},
  journal   = {Mathematics of Control, Signals and Systems},
  volume    = {2},
  number    = {4},
  pages     = {303--314},
  year      = {1989},
  publisher = {Springer}
}

@inproceedings{he2016deep,
  title     = {Deep Residual Learning for Image Recognition},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
  pages     = {770--778},
  year      = {2016}
}

@inproceedings{edelman2022inductive,
  title     = {Inductive Biases and Variable Creation in Self-Attention Mechanisms},
  author    = {Edelman, Benjamin L. and Goel, Surbhi and Kakade, Sham and Zhang, Cyril},
  booktitle = {International Conference on Machine Learning ({ICML})},
  pages     = {5793--5831},
  year      = {2022},
  publisher = {{PMLR}}
}

@misc{huggingface2024llm,
  author       = {{Hugging Face}},
  title        = {Large Language Models Course -- Chapter 1: Which Architecture to Choose?},
  year         = {2024},
  howpublished = {\url{https://huggingface.co/learn/llm-course/en/chapter1/6?fw=pt}},
  note         = {Accessed: 2025-09-22}
}

@article{hestness2017deep,
  title      = {Deep Learning Scaling is Predictable, Empirically},
  author     = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  journal    = {arXiv},
  eprint     = {1712.00409},
  eprinttype = {arXiv},
  year       = {2017}
}

@article{hoffmann2022training,
  title      = {Training Compute-Optimal Large Language Models},
  author     = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal    = {arXiv},
  eprint     = {2203.15556},
  eprinttype = {arXiv},
  year       = {2022}
}

@inbook{xiao2025foundations,
  title     = {Foundations of Large Language Models},
  author    = {Xiao, Tong and Zhu, Jingbo},
  pages     = {36--50},
  year      = {2025},
  publisher = {{NLP} Lab, Northeastern University \& NiuTrans Research}
}

@article{zhao2023survey,
  title      = {A Survey of Large Language Models},
  author     = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal    = {arXiv},
  eprint     = {2303.18223},
  eprinttype = {arXiv},
  year       = {2023}
}

@article{kaplan2020scaling,
  title      = {Scaling Laws for Neural Language Models},
  author     = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal    = {arXiv},
  eprint     = {2001.08361},
  eprinttype = {arXiv},
  year       = {2020}
}

@inproceedings{rajbhandari2020zero,
  title     = {{ZeRO}: Memory Optimizations Toward Training Trillion Parameter Models},
  author    = {Rajbhandari, Samyam and Ruwase, Olatunji and Li, Shaden and He, Yuxiong},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis ({SC}'20)},
  year      = {2020},
  publisher = {IEEE},
  doi       = {10.1109/SC41405.2020.00024}
}

@inproceedings{shoeybi2019megatron,
  title     = {{Megatron-LM}: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author    = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  booktitle = {Proceedings of the International Conference on Machine Learning ({ICML})},
  year      = {2019},
  url       = {https://arxiv.org/abs/1909.08053}
}

@article{goyal2017accurate,
  title      = {Accurate, Large Minibatch {SGD}: Training {ImageNet} in 1 Hour},
  author     = {Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal    = {arXiv},
  eprint     = {1706.02677},
  eprinttype = {arXiv},
  year       = {2017}
}

@inproceedings{narayanan2021efficient,
  title     = {Efficient Large-Scale Language Model Training on {GPU} Clusters},
  author    = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Daniel and Kashinkunti, Prabhat and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis ({SC}'21)},
  year      = {2021},
  doi       = {10.1145/3458817.3480834}
}

@article{patterson2021carbon,
  title      = {Carbon Emissions and Large Neural Network Training},
  author     = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Llu{\'\i}s and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal    = {arXiv},
  eprint     = {2104.10350},
  eprinttype = {arXiv},
  year       = {2021}
}

@article{ba2016layer,
  title      = {Layer Normalization},
  author     = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  journal    = {arXiv},
  eprint     = {1607.06450},
  eprinttype = {arXiv},
  year       = {2016}
}

@inproceedings{hu2021lora,
  title     = {{LoRA}: Low-Rank Adaptation of Large Language Models},
  author    = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle = {International Conference on Learning Representations ({ICLR})},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.09685}
}

@inproceedings{dettmers2023qlora,
  title     = {{QLoRA}: Efficient Finetuning of Quantized {LLM}s},
  author    = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  booktitle = {Advances in Neural Information Processing Systems ({NeurIPS})},
  year      = {2023},
  url       = {https://arxiv.org/abs/2305.14314}
}

@inproceedings{ouyang2022training,
  title     = {Training Language Models to Follow Instructions with Human Feedback},
  author    = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  booktitle = {Advances in Neural Information Processing Systems ({NeurIPS})},
  year      = {2022},
  url       = {https://arxiv.org/abs/2203.02155}
}

@inproceedings{howard2018universal,
  title     = {Universal Language Model Fine-Tuning for Text Classification},
  author    = {Howard, Jeremy and Ruder, Sebastian},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ({ACL})},
  pages     = {328--339},
  year      = {2018},
  url       = {https://arxiv.org/abs/1801.06146}
}
